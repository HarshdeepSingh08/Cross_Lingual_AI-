{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4f0adb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Checking audio file...\n",
      "âœ… File found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# Input audio file\n",
    "filename = \"ai_project_audio.mp3\"\n",
    "\n",
    "# Check if the file exists\n",
    "print(\"ðŸ” Checking audio file...\")\n",
    "if not os.path.exists(filename):\n",
    "    raise FileNotFoundError(f\"âŒ File not found: {filename}\")\n",
    "else:\n",
    "    print(\"âœ… File found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b742141c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ï¸ Loading Whisper model...\n",
      "ðŸ“ Transcribing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsd98\\anaconda3\\envs\\coqui_env\\lib\\site-packages\\whisper\\transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Transcript:  Rhea had no cake on her birthday. She felt sad. Her little brother drew a cake on paper. When she saw this, she felt very happy.\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "# Load Whisper model\n",
    "print(\"ðŸŽ™ï¸ Loading Whisper model...\")\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# Transcribe audio\n",
    "print(\"ðŸ“ Transcribing...\")\n",
    "result = model.transcribe(filename, language=\"en\")\n",
    "transcript = result[\"text\"]\n",
    "print(\"âœ… Transcript:\", transcript)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "083ac593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸˆ¶ Translated Text (Hindi): à¤°à¤¿à¤¯à¤¾ à¤•à¥‡ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤ªà¤° à¤•à¥‹à¤ˆ à¤•à¥‡à¤• à¤¨à¤¹à¥€à¤‚ à¤¥à¤¾à¥¤à¤µà¤¹ à¤¦à¥à¤–à¥€ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤° à¤°à¤¹à¥€ à¤¥à¥€à¥¤à¤‰à¤¸à¤•à¥‡ à¤›à¥‹à¤Ÿà¥‡ à¤­à¤¾à¤ˆ à¤¨à¥‡ à¤•à¤¾à¤—à¤œ à¤ªà¤° à¤à¤• à¤•à¥‡à¤• à¤–à¥€à¤‚à¤šà¤¾à¥¤à¤œà¤¬ à¤‰à¤¸à¤¨à¥‡ à¤¯à¤¹ à¤¦à¥‡à¤–à¤¾, à¤¤à¥‹ à¤‰à¤¸à¥‡ à¤¬à¤¹à¥à¤¤ à¤–à¥à¤¶à¥€ à¤¹à¥à¤ˆà¥¤\n"
     ]
    }
   ],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Translate transcript to Hindi\n",
    "translator = Translator()\n",
    "translated = translator.translate(transcript, src='en', dest='hi')\n",
    "translated_text = translated.text\n",
    "print(\"ðŸˆ¶ Translated Text (Hindi):\", translated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdfa8d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved reference audio: reference.wav\n"
     ]
    }
   ],
   "source": [
    "# Extract a short 10-second voice sample for cloning\n",
    "clip = AudioSegment.from_mp3(filename)[:5000]  \n",
    "ref_clip_path = \"reference.wav\"\n",
    "clip.export(ref_clip_path, format=\"wav\")\n",
    "print(f\"âœ… Saved reference audio: {ref_clip_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bd12253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ­ Emotion features: {'avg_pitch': 468.3760032860025, 'pitch_variance': 302111.39765560115, 'energy': 0.09959428012371063}\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load audio\n",
    "y, sr = librosa.load(filename)\n",
    "\n",
    "# Compute pitch (YIN) and energy (RMS)\n",
    "pitch = librosa.yin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "energy = np.mean(librosa.feature.rms(y=y))\n",
    "\n",
    "# Store emotion embedding\n",
    "avg_pitch = float(np.mean(pitch))\n",
    "pitch_var = float(np.var(pitch))\n",
    "energy_val = float(energy)\n",
    "\n",
    "emotion_embedding = {\n",
    "    \"avg_pitch\": avg_pitch,\n",
    "    \"pitch_variance\": pitch_var,\n",
    "    \"energy\": energy_val\n",
    "}\n",
    "print(\"ðŸŽ­ Emotion features:\", emotion_embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47a634ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hsd98\\anaconda3\\envs\\coqui_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['à¤°à¤¿à¤¯à¤¾ à¤•à¥‡ à¤œà¤¨à¥à¤®à¤¦à¤¿à¤¨ à¤ªà¤° à¤•à¥‹à¤ˆ à¤•à¥‡à¤• à¤¨à¤¹à¥€à¤‚ à¤¥à¤¾à¥¤à¤µà¤¹ à¤¦à¥à¤–à¥€ à¤®à¤¹à¤¸à¥‚à¤¸ à¤•à¤° à¤°à¤¹à¥€ à¤¥à¥€à¥¤à¤‰à¤¸à¤•à¥‡ à¤›à¥‹à¤Ÿà¥‡ à¤­à¤¾à¤ˆ à¤¨à¥‡ à¤•à¤¾à¤—à¤œ à¤ªà¤° à¤à¤• à¤•à¥‡à¤• à¤–à¥€à¤‚à¤šà¤¾à¥¤à¤œà¤¬ à¤‰à¤¸à¤¨à¥‡ à¤¯à¤¹ à¤¦à¥‡à¤–à¤¾, à¤¤à¥‹ à¤‰à¤¸à¥‡ à¤¬à¤¹à¥à¤¤ à¤–à¥à¤¶à¥€ à¤¹à¥à¤ˆà¥¤']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Processing time: 39.406291484832764\n",
      " > Real-time factor: 3.0826358320108502\n",
      "âœ… Raw XTTS output saved: output_xtts_raw.wav\n"
     ]
    }
   ],
   "source": [
    "from torch.serialization import add_safe_globals\n",
    "\n",
    "# XTTS-related configs and models\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "from TTS.tts.models.xtts import XttsAudioConfig, XttsArgs\n",
    "from TTS.config.shared_configs import BaseDatasetConfig\n",
    "\n",
    "# Add all necessary safe globals for Coqui XTTS\n",
    "add_safe_globals([\n",
    "    XttsConfig,\n",
    "    XttsAudioConfig,\n",
    "    XttsArgs,\n",
    "    BaseDatasetConfig,\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from TTS.api import TTS\n",
    "\n",
    "# Load Coqui XTTS model\n",
    "tts = TTS(model_name=\"tts_models/multilingual/multi-dataset/xtts_v2\", progress_bar=True, gpu=False)\n",
    "\n",
    "# Generate initial Hindi speech\n",
    "intermediate_audio = \"output_xtts_raw.wav\"\n",
    "tts.tts_to_file(\n",
    "    text=translated_text,\n",
    "    speaker_wav=ref_clip_path,\n",
    "    language=\"hi\",\n",
    "    file_path=intermediate_audio\n",
    ")\n",
    "print(\"âœ… Raw XTTS output saved:\", intermediate_audio)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf225b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Pitch shift (semitones): 0.50\n",
      "âœ… Final audio with smooth emotion modulation saved as: output_xtts_emotion.wav\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import pyrubberband as pyrb\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "# Load audio\n",
    "y_out, sr_out = librosa.load(\"output_xtts_raw.wav\", sr=None)\n",
    "y_out = y_out.astype(np.float32)\n",
    "\n",
    "# === Pitch shift (very subtle) ===\n",
    "baseline_pitch = 150\n",
    "semitone_shift = np.log2(avg_pitch / baseline_pitch) * 12\n",
    "semitone_shift = float(np.clip(semitone_shift, -0.5, 0.5))  # subtle and capped\n",
    "print(f\"ðŸ”§ Pitch shift (semitones): {semitone_shift:.2f}\")\n",
    "\n",
    "y_shifted = pyrb.pitch_shift(y_out, sr_out, n_steps=semitone_shift)\n",
    "\n",
    "# === Smooth energy envelope scaling ===\n",
    "frame_length = 2048\n",
    "hop_length = 512\n",
    "\n",
    "# Extract frame-wise RMS energy and smooth it\n",
    "rms = librosa.feature.rms(y=y_shifted, frame_length=frame_length, hop_length=hop_length)[0]\n",
    "rms_smooth = gaussian_filter1d(rms, sigma=3)\n",
    "\n",
    "# Target energy factor (relative to neutral reference)\n",
    "target_rms = energy_val  # from emotion embedding\n",
    "ref_rms = 0.05\n",
    "scaling_curve = np.clip(np.sqrt(target_rms / ref_rms) * (rms_smooth / np.mean(rms_smooth)), 0.5, 2.0)\n",
    "\n",
    "# Map frame-wise scaling back to full signal length\n",
    "frame_centers = librosa.frames_to_samples(np.arange(len(scaling_curve)), hop_length=hop_length)\n",
    "scaling_envelope = np.interp(np.arange(len(y_shifted)), frame_centers, scaling_curve)\n",
    "\n",
    "# Apply the smoothed gain envelope\n",
    "y_final = y_shifted * scaling_envelope\n",
    "y_final = np.clip(y_final, -1.0, 1.0)  # avoid clipping\n",
    "\n",
    "# Save output\n",
    "final_output = \"output_xtts_emotion.wav\"\n",
    "sf.write(final_output, y_final, sr_out)\n",
    "print(\"âœ… Final audio with smooth emotion modulation saved as:\", final_output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coqui_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
